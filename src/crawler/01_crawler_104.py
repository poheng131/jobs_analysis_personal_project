# -*- coding: utf-8 -*-
"""
Created on Tue Jun 17 13:07:35 2025

@author: student
"""

import requests
import time
import pandas as pd
from datetime import datetime
from urllib.parse import quote

# âœ… ç™½åå–®ï¼šåªä¿ç•™é€™äº›è·ç¼ºåç¨±é—œéµå­—çš„è·ç¼º
WHITELIST_KEYWORDS = [
    "æ•¸æ“šåˆ†æ", "è³‡æ–™åˆ†æ", "data analyst", "data analysis", 
    "data analytic", "è³‡æ–™ç§‘å­¸", "data scientist",
    "è³‡æ–™å·¥ç¨‹", "data engineer", "å•†æ¥­åˆ†æ", "bi", 
    "biå·¥ç¨‹å¸«", "bi analyst", "powerbi", "business intelligence",
    "business analyst", "machine learning", "AIåˆ†æ"
]

# âŒ é»‘åå–®ï¼šæ’é™¤é€™äº›æ˜é¡¯ä¸ç›¸é—œçš„è·ç¼º
EXCLUDE_WORDS = ["åŠ©ç†", "å®¢æœ", "é–€å¸‚", "å„²å‚™å¹¹éƒ¨", "å·¥è®€", "è¬›å¸«", "ä½œæ¥­å“¡", "è¡Œæ”¿", "æ¥­å‹™", "å¤–åŒ…", "è¨­è¨ˆ"]

def is_relevant_job(title):
    title = title.lower()
    return any(good in title for good in WHITELIST_KEYWORDS) and not any(bad in title for bad in EXCLUDE_WORDS)

# âœ… ä¸»çˆ¬èŸ²å‡½å¼
def get_104_jobs_raw(keyword, max_pages=100):
    headers = {
        "User-Agent": "Mozilla/5.0",
        "Referer": "https://www.104.com.tw/jobs/search/",
    }
    jobs = []
    page = 1

    while page <= max_pages:
        url = (
            "https://www.104.com.tw/jobs/search/list?"
            f"ro=0&kwop=7&keyword={quote(keyword)}&order=11&asc=0&page={page}&mode=s&jobsource=2018indexpoc"
        )

        print(f"ğŸ” æŠ“å–ã€Œ{keyword}ã€ç¬¬ {page} é ")
        resp = requests.get(url, headers=headers)
        try:
            data = resp.json()
        except Exception:
            print("âš ï¸ ç„¡æ³•è§£æ JSONï¼Œè·³é")
            break

        if "data" not in data or "list" not in data["data"]:
            print("âš ï¸ è³‡æ–™æ ¼å¼éŒ¯èª¤ï¼ŒçµæŸ")
            break

        job_list = data["data"]["list"]
        if not job_list:
            print("âœ… ç„¡æ›´å¤šè·ç¼º")
            break

        today = datetime.today().strftime("%Y-%m-%d")
        for job in job_list:
            title = job.get("jobName", "")
            if is_relevant_job(title):
                job["çˆ¬èŸ²æ—¥æœŸ"] = today
                jobs.append(job)

        print(f"âœ… ç¬¬ {page} é å®Œæˆï¼Œå…±ç´¯ç© {len(jobs)} ç­†")
        page += 1
        time.sleep(1)

    return jobs

# âœ… ä¸»æµç¨‹
if __name__ == "__main__":
    all_jobs = []
    # ğŸ‘‰ é—œéµå­—å¯åªè¨­ç‚º "è³‡æ–™"ã€"æ•¸æ“š"ï¼Œå› ç‚ºç¯©é¸é‚è¼¯æ”¹ç”± is_relevant_job æ§åˆ¶
    SEARCH_KEYWORDS = ["æ•¸æ“š", "è³‡æ–™", "åˆ†æ", "data"]

    for kw in SEARCH_KEYWORDS:
        jobs = get_104_jobs_raw(kw, max_pages=100)
        all_jobs.extend(jobs)

    df = pd.DataFrame(all_jobs)

    # âœ… å»é™¤é‡è¤‡è·ç¼ºï¼ˆæ ¹æ“šè·ç¼ºåç¨±ã€å…¬å¸åç¨±ã€è·ç¼ºç·¨è™Ÿï¼‰
    df.drop_duplicates(subset=["jobName", "custName", "jobNo"], inplace=True)

    # âœ… å„²å­˜è³‡æ–™
    today = datetime.today().strftime("%Y-%m-%d")
    filename = f"104_jobs_filtered_{today}.csv"
    df.to_csv(filename, index=False, encoding="utf-8-sig")

    print(f"\nğŸ‰ å®Œæˆï¼å…±å„²å­˜ {len(df)} ç­†ç¬¦åˆç™½åå–®çš„è·ç¼º âœ {filename}")
